#!/usr/bin/env python3
"""
Quick test script to verify data parsing from local files.

This script tests the APIExtractor to ensure it correctly parses:
- CVE ID, description, summary, justification
- Checklist questions and responses
- Intel score and breakdown
- Investigation action logs

Usage:
    python test_data_parser.py
"""

import json
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from evaluation.extractors.data_extractor import APIExtractor


def main():
    """Test data parsing."""
    print("=" * 80)
    print("Testing Data Parser")
    print("=" * 80)
    print()

    # Load test data
    jobs_file = "src/evaluation/test_data/jobs_integration_test_all.json"
    traces_file = "src/evaluation/test_data/traces_integration_test_all.json"

    print("Loading test data...")
    print(f"   Jobs: {jobs_file}")
    print(f"   Traces: {traces_file}")
    print()

    with open(jobs_file, 'r') as f:
        jobs = json.load(f)

    with open(traces_file, 'r') as f:
        traces = json.load(f)

    print(f"Loaded {len(jobs)} jobs, {len(traces)} traces")
    print()

    # Group traces by job_id
    traces_by_job = {}
    for trace in traces:
        job_id = trace.get("job_id", "")
        if job_id:
            if job_id not in traces_by_job:
                traces_by_job[job_id] = []
            traces_by_job[job_id].append(trace)

    print(f"Grouped traces into {len(traces_by_job)} job groups")
    print()

    # Test parsing first job
    job = jobs[0]
    job_id = job.get("job_id", "")
    job_traces = traces_by_job.get(job_id, [])

    print("=" * 80)
    print(f"Testing Parser on Job: {job_id}")
    print("=" * 80)
    print()

    if not job_traces:
        print(f"ERROR: No traces found for job {job_id}")
        return 1

    print(f"Found {len(job_traces)} traces for this job")
    print()

    # Parse
    print("ðŸ”„ Parsing data...")
    parsed = APIExtractor.extract_from_job(job, job_traces)

    if not parsed:
        print("ERROR: Parsing failed!")
        return 1

    print("Parsing successful!")
    print()

    # Display results
    print("=" * 80)
    print("Parsed Data Summary")
    print("=" * 80)
    print()

    print(f"ðŸ“Œ Basic Info:")
    print(f"   CVE ID: {parsed.cve_id}")
    print(f"   Job ID: {parsed.job_id}")
    print(f"   Intel Score: {parsed.intel_score}")
    print()

    print("CVE Description:")
    if parsed.description:
        preview = parsed.description[:200] + "..." if len(parsed.description) > 200 else parsed.description
        print(f"   {preview}")
    else:
        print("   (empty)")
    print()

    print(f"ðŸ“‹ Checklist:")
    print(f"   Questions: {len(parsed.checklist_items)}")
    for i, q in enumerate(parsed.checklist_items[:3], 1):
        print(f"   {i}. {q[:80]}...")
    if len(parsed.checklist_items) > 3:
        print(f"   ... and {len(parsed.checklist_items) - 3} more")
    print()

    print("Investigation:")
    print(f"   Steps with details: {len(parsed.checklist_step_details)}")
    for i, step in enumerate(parsed.checklist_step_details[:30], 1):
        print(f"   Step {i}: {len(step.tool_calls)} tool calls")
        print(step)
        if step.tool_calls:
            print(f"      Tools used: {', '.join(set(tc.tool_name for tc in step.tool_calls))}")
    if len(parsed.checklist_step_details) > 2:
        print(f"   ... and {len(parsed.checklist_step_details) - 2} more steps")
    print()

    print("Summary:")
    if parsed.summary:
        preview = parsed.summary[:200] + "..." if len(parsed.summary) > 200 else parsed.summary
        print(f"   {preview}")
    else:
        print("   (empty)")
    print()

    print(f"âš–ï¸  Justification:")
    print(f"   Label: {parsed.justification_label}")
    if parsed.justification_reason:
        preview = parsed.justification_reason[:150] + "..." if len(
            parsed.justification_reason) > 150 else parsed.justification_reason
        print(f"   Reason: {preview}")
    print()

    # Check for intel score breakdown
    intel_breakdown = parsed.__dict__.get("intel_score_breakdown")
    intel_justifications = parsed.__dict__.get("intel_score_justifications")

    if intel_breakdown:
        print("Intel Score Breakdown:")
        for dim, score in intel_breakdown.items():
            print(f"   {dim}: {score}")
        print()

        if intel_justifications:
            print("Intel Score Justifications:")
            for dim, justification in list(intel_justifications.items())[:8]:
                preview = justification[:100] + "..." if len(justification) > 100 else justification
                print(f"   {dim}: {preview}")
            print()
    else:
        print("WARNING: Intel score breakdown not found in traces")
        print()

    # Save parsed data to JSON for inspection
    print("=" * 80)
    print("Saving parsed data to JSON...")
    print("=" * 80)
    output_file = "parsed_evaluation_input.json"

    # Convert parsed result to dict for JSON serialization
    parsed_dict = {
        "cve_id": parsed.cve_id,
        "job_id": parsed.job_id,
        "trace_id": parsed.trace_id,
        "execution_start_timestamp": parsed.execution_start_timestamp,

  # CVE Info
        "cve_description": parsed.description,

  # Checklist (for checklist evaluation)
        "checklist_questions": parsed.checklist_items,
        "checklist_responses": [{
            "question": step.question, "response": step.response
        } for step in parsed.checklist_step_details],
        # "checklist_responses": [{"question": step.question, "response": step.response} for step in parsed.checklist_steps],

        # Investigation (for investigation evaluation)
        "investigation_steps": [{
            "step_number":
                step.step_number,
            "question":
                step.question,
            "response":
                step.response,
            "tool_calls": [{
                "tool_name":
                    tc.tool_name,
                "tool_input":
                    tc.tool_input,
                "observation":
                    tc.tool_output[:200] + "..." if tc.tool_output and len(tc.tool_output) > 200 else tc.tool_output
            } for tc in step.tool_calls]
        } for step in parsed.checklist_step_details],

  # Summary (for summary evaluation)
        "summary": parsed.summary,

  # Justification (for justification evaluation)
        "justification": {
            "label": parsed.justification_label, "reason": parsed.justification_reason
        },

  # Intel Score (for intel score evaluation)
        "intel_score": {
            "total": parsed.intel_score,
            "breakdown": intel_breakdown if intel_breakdown else {},
            "justifications": intel_justifications if intel_justifications else {}
        }
    }

    with open(output_file, 'w') as f:
        json.dump(parsed_dict, f, indent=2, ensure_ascii=False)

    print(f"Saved to: {output_file}")
    print()
    print("Inspect the exact input data that will be used for evaluation:")
    print(f"   jq . {output_file}")
    print(f"   jq '.investigation_steps' {output_file}  # View investigation details")
    print(f"   jq '.intel_score' {output_file}  # View intel score breakdown")
    print()

    print("=" * 80)
    print("Test Complete!")
    print("=" * 80)
    print()
    print("Next steps:")
    print("  1. If data looks good, try running full evaluation:")
    print("     python run_cve_evaluation.py --mode local --limit 1 --no-submit")
    print()
    print("  2. Then test with API:")
    print("     export BASE='your-api-url'")
    print("     export TOKEN='your-token'")
    print("     python run_cve_evaluation.py --mode api --limit 1 --no-submit")
    print()

    return 0


if __name__ == "__main__":
    sys.exit(main())
