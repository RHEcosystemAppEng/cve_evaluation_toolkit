#!/usr/bin/env python3
# SPDX-FileCopyrightText: Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
"""
Complete CVE Evaluation Pipeline Runner.

This script orchestrates the full evaluation pipeline:
1. Fetches jobs and traces from remote cluster API (or loads from local files)
2. Parses nested JSON data using APIExtractor
3. Runs all evaluation metrics on parsed data
4. Submits evaluation results back to API (or saves locally)

Usage:
    # From API with auto-submit
    export BASE="https://your-api-endpoint.com"
    export TOKEN="your-token"
    python run_cve_evaluation.py --mode api --limit 5

    # From API without submit (dry-run)
    python run_cve_evaluation.py --mode api --no-submit --output eval_results.json

    # From local files for testing
    python run_cve_evaluation.py --mode local \
        --jobs-file src/evaluation/test_data/jobs.json \
        --traces-file api_traces_sample.json \
        --no-submit
"""

import argparse
import asyncio
import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Optional

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

from evaluation.api_client import APIConfig, ExploitIQClient
from evaluation.extractors.data_extractor import APIExtractor, CVEAnalysisResult
from evaluation.metrics.agent import (
    ChecklistMetricSuite,
    ChecklistEvalInput,
    InvestigationMetricSuite,
    InvestigationEvalInput,
    SummaryMetricSuite,
    SummaryEvalInput,
    JustificationMetricSuite,
    JustificationEvalInput,
    IntelScoreMetricSuite,
    IntelScoreEvalInput,
)
from evaluation.models import FireworksJudge, JudgeConfig

# Logger with compatibility for both old and new structure
try:
    from evaluation.utils.logger import get_logger
    logger = get_logger(__name__)
except ImportError:
    # Fallback to vuln_analysis logger
    from vuln_analysis.logging.loggers_factory import LoggingFactory
    logger = LoggingFactory.get_agent_logger(__name__)


class CVEEvaluationOrchestrator:
    """
    Orchestrates the complete CVE evaluation pipeline.

    Responsibilities:
    - Fetch jobs and traces from API
    - Parse nested JSON data
    - Run evaluation metrics
    - Submit results back to API
    """

    def __init__(self, api_client: ExploitIQClient, judge_model: Any):
        """
        Initialize orchestrator.

        Args:
            api_client: ExploitIQClient for API communication
            judge_model: LLM judge model for evaluation
        """
        self.api_client = api_client
        self.judge_model = judge_model

        # Initialize evaluation metric suites
        logger.info("Initializing evaluation metric suites...")
        self.checklist_suite = ChecklistMetricSuite(judge_model)
        self.investigation_suite = InvestigationMetricSuite(judge_model)
        self.summary_suite = SummaryMetricSuite(judge_model)
        self.justification_suite = JustificationMetricSuite(judge_model)
        self.intel_score_suite = IntelScoreMetricSuite(judge_model)

        logger.info("CVEEvaluationOrchestrator initialized with all metric suites")

    async def fetch_and_parse_job(self, job: dict[str, Any]) -> Optional[CVEAnalysisResult]:
        """
        Fetch traces for a job and parse all evaluation data.

        Args:
            job: Job dictionary from API

        Returns:
            CVEAnalysisResult with parsed data, or None if parsing fails or no traces found
        """
        try:
            job_id = job.get("job_id", "")
            cve_id = job.get("cve", "")

            logger.info("=" * 80)
            logger.info("Processing job: %s (CVE: %s)", job_id, cve_id)
            logger.info("=" * 80)

            # Fetch traces for this job
            logger.info("  Fetching traces...")
            traces = await self.api_client.fetch_traces(job_id)

            if not traces or len(traces) == 0:
                logger.warning("  WARNING: No traces found for job %s, skipping to next job...", job_id)
                return None

            logger.info("  Fetched %d traces", len(traces))

            # Parse job and traces using APIExtractor
            logger.info("  Parsing nested JSON data...")
            parsed_result = APIExtractor.extract_from_job(job, traces)

            if not parsed_result:
                logger.error("  ERROR: Failed to parse job %s", job_id)
                return None

            logger.info("  Successfully parsed:")
            logger.info("     - CVE: %s", parsed_result.cve_id)
            logger.info("     - Checklist questions: %d", len(parsed_result.checklist_items))
            logger.info("     - Investigation steps: %d", len(parsed_result.checklist_step_details))
            logger.info("     - Intel score: %s", parsed_result.intel_score)

            return parsed_result

        except Exception as e:
            logger.error("ERROR: Failed to fetch and parse job %s: %s", job.get("job_id"), e, exc_info=True)
            return None

    def evaluate_cve_analysis(self, parsed_result: CVEAnalysisResult, stages: list[str] = None) -> dict[str, Any]:
        """
        Run evaluation metrics on parsed CVE analysis.

        Args:
            parsed_result: Parsed CVE analysis data
            stages: List of stages to evaluate. If None or ["all"], evaluates all stages.
                   Options: "checklist", "investigation", "summary", "justification", "intel_score"

        Returns:
            Dictionary with evaluation results for all metrics
        """
        # Normalize stages
        if stages is None or "all" in stages:
            stages = ["checklist", "investigation", "summary", "justification", "intel_score"]

        logger.info("")
        logger.info("Running evaluation metrics for CVE %s (stages: %s)...",
                   parsed_result.cve_id, ", ".join(stages))

        evaluation_results = {
            "cve_id": parsed_result.cve_id,
            "job_id": parsed_result.job_id,
            "trace_id": parsed_result.trace_id or "unknown",
            "execution_start_timestamp": parsed_result.execution_start_timestamp or datetime.now().isoformat(),
            "component": parsed_result.component,
            "component_version": parsed_result.component_version,
            "timestamp": datetime.now().isoformat(),
            "stages": stages,
            "metrics": []
        }

        # 1. Checklist Evaluation
        if "checklist" in stages and parsed_result.checklist_items:
            try:
                logger.info("Running checklist evaluation...")
                checklist_input = ChecklistEvalInput(
                    cve_id=parsed_result.cve_id,
                    cve_description=parsed_result.description,
                    checklist_items=parsed_result.checklist_items
                )
                checklist_results = self.checklist_suite.evaluate(checklist_input)

                # Convert to API format
                for metric_name, result in checklist_results.get("individual_results", {}).items():
                    # Map metric names to API format
                    api_metric_name = metric_name.upper().replace(" ", "_")
                    if api_metric_name == "PROMPT_ALIGNMENT":
                        api_metric_name = "CHECKLIST_PROMPT_ALIGNMENT"

                    evaluation_results["metrics"].append({
                        "llm_node": "CHECKLIST_GENERATION",
                        "metric_name": api_metric_name,
                        "metric_score": result.get("score", 0.0),
                        "metric_reasoning": result.get("reason", "")
                    })

                logger.info("  Checklist: overall=%.2f", checklist_results.get("overall_score", 0))
            except Exception as e:
                logger.error("  ERROR: Checklist evaluation failed: %s", e, exc_info=True)

        # 2. Investigation Evaluation (per step)
        if "investigation" in stages and parsed_result.checklist_step_details:
            try:
                logger.info("Running investigation evaluation (%d steps)...",
                           len(parsed_result.checklist_step_details))

                investigation_scores = []
                for i, step_detail in enumerate(parsed_result.checklist_step_details, 1):
                    try:
                        eval_input = InvestigationEvalInput.from_step_detail(
                            parsed_result.cve_id,
                            step_detail
                        )
                        step_result = self.investigation_suite.evaluate(eval_input)
                        investigation_scores.append(step_result.get("overall_score", 0))

                        # Add metrics from this step
                        for metric_name, result in step_result.get("individual_results", {}).items():
                            # Map metric names to API format with AGENT_LOOP prefix
                            api_metric_name = metric_name.upper().replace(" ", "_")
                            api_metric_name = f"AGENT_LOOP_{api_metric_name}"

                            evaluation_results["metrics"].append({
                                "llm_node": "AGENT_LOOP",
                                "metric_name": api_metric_name,
                                "metric_score": result.get("score", 0.0),
                                "metric_reasoning": result.get("reason", ""),
                                "model_input": step_detail.question,
                                "model_output": step_detail.response
                            })
                    except Exception as e:
                        logger.warning("    Step %d failed: %s", i, e)

                avg_score = sum(investigation_scores) / len(investigation_scores) if investigation_scores else 0
                logger.info("  Investigation: avg=%.2f (%d steps)", avg_score, len(investigation_scores))
            except Exception as e:
                logger.error("  ERROR: Investigation evaluation failed: %s", e, exc_info=True)

        # 3. Summary Evaluation
        if "summary" in stages and parsed_result.summary:
            try:
                logger.info("Running summary evaluation...")
                summary_input = SummaryEvalInput.from_extraction(
                    cve_id=parsed_result.cve_id,
                    cve_description=parsed_result.description,
                    summary=parsed_result.summary,
                    checklist_step_details=parsed_result.checklist_step_details
                )
                summary_results = self.summary_suite.evaluate(summary_input)

                evaluation_results["metrics"].append({
                    "llm_node": "SUMMARIZE",
                    "metric_name": "SUMMARY_QUALITY",
                    "metric_score": summary_results.get("overall_score", 0.0),
                    "metric_reasoning": summary_results.get("reason", "")
                })

                logger.info("  Summary: score=%.2f", summary_results.get("overall_score", 0))
            except Exception as e:
                logger.error("  ERROR: Summary evaluation failed: %s", e, exc_info=True)

        # 4. Justification Evaluation
        if "justification" in stages and parsed_result.justification_label:
            try:
                logger.info("Running justification evaluation...")
                justification_input = JustificationEvalInput.from_extraction(
                    cve_id=parsed_result.cve_id,
                    justification_label=parsed_result.justification_label,
                    justification_reason=parsed_result.justification_reason,
                    summary=parsed_result.summary
                )
                justification_results = self.justification_suite.evaluate(justification_input)

                evaluation_results["metrics"].append({
                    "llm_node": "JUSTIFICATION",
                    "metric_name": "JUSTIFICATION_QUALITY",
                    "metric_score": justification_results.get("overall_score", 0.0),
                    "metric_reasoning": justification_results.get("reason", "")
                })

                logger.info("  Justification: score=%.2f", justification_results.get("overall_score", 0))
            except Exception as e:
                logger.error("  ERROR: Justification evaluation failed: %s", e, exc_info=True)

        # 5. Intel Score Evaluation
        if "intel_score" in stages and parsed_result.intel_score is not None:
            try:
                logger.info("Running intel score evaluation...")

                # Extract intel score breakdown from parsed result
                intel_breakdown = parsed_result.__dict__.get("intel_score_breakdown", {})
                intel_justifications = parsed_result.__dict__.get("intel_score_justifications", {})

                if intel_breakdown and intel_justifications:
                    intel_input = IntelScoreEvalInput(
                        cve_id=parsed_result.cve_id,
                        cve_description=parsed_result.description,
                        scores=intel_breakdown,
                        justifications=intel_justifications,
                        total_score=parsed_result.intel_score
                    )
                    intel_results = self.intel_score_suite.evaluate(intel_input)

                    # Add metrics
                    for metric_name, result in intel_results.get("individual_results", {}).items():
                        evaluation_results["metrics"].append({
                            "llm_node": "CALCULATE_CVE_SCORE",
                            "metric_name": metric_name.upper().replace(" ", "_"),
                            "metric_score": result.get("score", 0.0),
                            "metric_reasoning": result.get("reason", "")
                        })

                    logger.info("  Intel Score: score=%.2f", intel_results.get("overall_score", 0))
                else:
                    logger.warning("  WARNING: Intel score breakdown not available in traces")
            except Exception as e:
                logger.error("  ERROR: Intel score evaluation failed: %s", e, exc_info=True)

        logger.info("")
        logger.info("Evaluation complete: %d metrics computed", len(evaluation_results["metrics"]))

        return evaluation_results

    async def submit_evaluation_results(
        self,
        parsed_result: CVEAnalysisResult,
        evaluation_results: dict[str, Any],
        trace_id: str,
        execution_start_timestamp: str
    ) -> bool:
        """
        Submit evaluation results back to API.

        Args:
            parsed_result: Parsed CVE analysis
            evaluation_results: Evaluation metrics results
            trace_id: Trace ID for this execution
            execution_start_timestamp: Execution start timestamp

        Returns:
            True if submission succeeded, False otherwise
        """
        try:
            logger.info(" Submitting evaluation results to API...")

            # Extract component info (may need adjustment based on actual data structure)
            component = parsed_result.__dict__.get("component", "unknown")
            component_version = parsed_result.__dict__.get("component_version", "unknown")

            # Submit to API
            response = await self.api_client.submit_evaluation(
                job_id=parsed_result.job_id,
                trace_id=trace_id,
                cve=parsed_result.cve_id,
                component=component,
                component_version=component_version,
                execution_start_timestamp=execution_start_timestamp,
                evaluation_results=evaluation_results["metrics"]
            )

            logger.info("Successfully submitted %d metrics", len(evaluation_results["metrics"]))
            return True

        except Exception as e:
            logger.error("ERROR: Failed to submit evaluation results: %s", e, exc_info=True)
            return False

    async def process_single_job(
        self,
        job: dict[str, Any],
        submit_results: bool = True,
        stages: list[str] = None
    ) -> Optional[dict[str, Any]]:
        """
        Complete pipeline for a single job: fetch, parse, evaluate, submit.

        Args:
            job: Job dictionary from API
            submit_results: Whether to submit results back to API
            stages: List of evaluation stages to run

        Returns:
            Evaluation results dictionary, or None if processing failed
        """
        try:
            # Step 1: Fetch and parse
            parsed_result = await self.fetch_and_parse_job(job)
            if not parsed_result:
                return None

            # Step 2: Evaluate
            evaluation_results = self.evaluate_cve_analysis(parsed_result, stages=stages)

            # Step 3: Submit results (if enabled)
            if submit_results:
                # Extract trace_id and timestamp from parsed_result (already extracted from traces)
                trace_id = parsed_result.trace_id or "unknown"
                execution_start_timestamp = parsed_result.execution_start_timestamp or datetime.now().isoformat()

                await self.submit_evaluation_results(
                    parsed_result,
                    evaluation_results,
                    trace_id,
                    execution_start_timestamp
                )

            return evaluation_results

        except Exception as e:
            logger.error("ERROR: Failed to process job %s: %s", job.get("job_id"), e, exc_info=True)
            return None

    async def process_batch(
        self,
        limit: Optional[int] = None,
        submit_results: bool = True,
        stages: list[str] = None,
        job_id: str = None
    ) -> list[dict[str, Any]]:
        """
        Process a batch of jobs from the API.

        Args:
            limit: Maximum number of jobs to process (None = all)
            submit_results: Whether to submit results back to API
            stages: List of evaluation stages to run
            job_id: Specific job_id to process (if provided, only processes this job)

        Returns:
            List of evaluation results for all processed jobs
        """
        logger.info("")
        logger.info("=" * 80)
        logger.info("Starting batch processing from API")
        logger.info("=" * 80)

        # Fetch jobs from API
        if job_id:
            logger.info("Fetching specific job: %s", job_id)
            try:
                job = await self.api_client.fetch_job_by_id(job_id)
                jobs = [job] if job else []
            except Exception as e:
                logger.error("Failed to fetch job %s: %s", job_id, e)
                return []
        else:
            logger.info("Fetching jobs from API...")
            jobs = await self.api_client.fetch_jobs(status="completed", limit=limit)

        logger.info("Fetched %d jobs", len(jobs))
        logger.info("")

        all_results = []
        successful = 0
        failed = 0
        skipped_no_traces = 0

        for i, job in enumerate(jobs, 1):
            logger.info("")
            logger.info("Processing job %d/%d", i, len(jobs))

            result = await self.process_single_job(job, submit_results=submit_results, stages=stages)

            if result:
                all_results.append(result)
                successful += 1
                logger.info("  Job completed successfully")
            else:
                # Check if it failed due to no traces or other error
                # For now, count as skipped/failed
                failed += 1
                logger.info("Job skipped or failed, continuing to next...")

        logger.info("")
        logger.info("=" * 80)
        logger.info("Batch processing complete")
        logger.info("   Total jobs: %d", len(jobs))
        logger.info("   Successful: %d", successful)
        logger.info("Skipped/Failed: %d", failed)
        logger.info("   Success rate: %.1f%%", (successful / len(jobs) * 100) if jobs else 0)
        logger.info("=" * 80)

        return all_results


async def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Complete CVE evaluation pipeline: fetch, parse, evaluate, submit",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # From API with auto-submit
  export BASE="https://your-api.com"
  export TOKEN="your-token"
  python run_cve_evaluation.py --mode api --limit 5

  # From API without submit (dry-run)
  python run_cve_evaluation.py --mode api --no-submit --output results.json

  # From local files for testing
  python run_cve_evaluation.py --mode local \\
      --jobs-file src/evaluation/test_data/jobs.json \\
      --traces-file api_traces_sample.json \\
      --no-submit
        """
    )

    parser.add_argument(
        "--mode",
        choices=["api", "local"],
        default="api",
        help="Run mode: fetch from API or use local test files"
    )
    parser.add_argument(
        "--limit",
        type=int,
        help="Limit number of jobs to process"
    )
    parser.add_argument(
        "--submit",
        action="store_true",
        default=True,
        help="Submit evaluation results back to API (default: True)"
    )
    parser.add_argument(
        "--no-submit",
        dest="submit",
        action="store_false",
        help="Don't submit results, only compute and save locally"
    )
    parser.add_argument(
        "--output",
        default="evaluation_results.json",
        help="Output file for evaluation results (default: evaluation_results.json)"
    )
    parser.add_argument(
        "--jobs-file",
        default="src/evaluation/test_data/jobs.json",
        help="Path to local jobs.json (local mode only)"
    )
    parser.add_argument(
        "--traces-file",
        default="api_traces_sample.json",
        help="Path to local traces.json (local mode only)"
    )
    parser.add_argument(
        "--stages",
        nargs="+",
        choices=["all", "checklist", "investigation", "summary", "justification", "intel_score"],
        default=["all"],
        help="Evaluation stages to run (default: all). Can specify multiple: --stages checklist summary"
    )
    parser.add_argument(
        "--job-id",
        type=str,
        help="Specific job_id to evaluate (optional). If not provided, processes recent completed jobs based on limit"
    )
    parser.add_argument(
        "--output-format",
        choices=["local", "api"],
        default="local",
        help="Output file format: 'local' (nested structure) or 'api' (flat list, same as POST format)"
    )

    args = parser.parse_args()

    # Check environment variables for API mode
    if args.mode == "api":
        base_url = os.getenv("BASE") or os.getenv("EXPLOIT_IQ_API_BASE")
        token = os.getenv("TOKEN") or os.getenv("EXPLOIT_IQ_API_TOKEN")

        if not base_url or not token:
            logger.error("ERROR: Missing API credentials!")
            logger.error("")
            logger.error("Please set environment variables:")
            logger.error("  export BASE='https://your-api-endpoint.com'")
            logger.error("  export TOKEN='your-token-here'")
            logger.error("")
            logger.error("Or alternatively:")
            logger.error("  export EXPLOIT_IQ_API_BASE='...'")
            logger.error("  export EXPLOIT_IQ_API_TOKEN='...'")
            return 1

        logger.info("API Configuration:")
        logger.info("  Base URL: %s", base_url)
        logger.info("  Token: %s***%s", token[:8] if len(token) > 12 else "***",
                   token[-4:] if len(token) > 4 else "")

    # Initialize API client
    logger.info("")
    logger.info("Initializing API client...")
    api_config = APIConfig()
    api_client = ExploitIQClient(config=api_config)

    # Initialize judge model
    logger.info("Initializing LLM judge model...")
    judge_config = JudgeConfig()
    judge_model = FireworksJudge(config=judge_config)
    logger.info("  Judge model: %s", judge_config.model_name)

    # Initialize orchestrator
    logger.info("")
    orchestrator = CVEEvaluationOrchestrator(api_client, judge_model)

    # Run pipeline
    try:
        if args.mode == "api":
            # Fetch from API and process
            results = await orchestrator.process_batch(
                limit=args.limit,
                submit_results=args.submit,
                stages=args.stages,
                job_id=args.job_id
            )

        else:
            # Local mode: load from files for testing
            logger.info("")
            logger.info("=" * 80)
            logger.info("Starting local mode (test files)")
            logger.info("=" * 80)
            logger.info("  Jobs file: %s", args.jobs_file)
            logger.info("  Traces file: %s", args.traces_file)

            jobs, traces = api_client.load_from_local_files(args.jobs_file, args.traces_file)

            # Group traces by job_id
            traces_by_job = {}
            for trace in traces:
                job_id = trace.get("job_id", "")
                if job_id:
                    if job_id not in traces_by_job:
                        traces_by_job[job_id] = []
                    traces_by_job[job_id].append(trace)

            logger.info("  Loaded %d jobs, %d traces", len(jobs), len(traces))
            logger.info("")

            # Process each job
            results = []
            jobs_to_process = jobs[:args.limit] if args.limit else jobs
            successful = 0
            skipped = 0

            for i, job in enumerate(jobs_to_process, 1):
                logger.info("")
                logger.info("Processing job %d/%d (local mode)", i, len(jobs_to_process))

                job_id = job.get("job_id", "")
                job_traces = traces_by_job.get(job_id, [])

                if not job_traces:
                    logger.warning("  WARNING: No traces found for job %s in local file, skipping to next...", job_id)
                    skipped += 1
                    continue

                # Parse
                parsed_result = APIExtractor.extract_from_job(job, job_traces)
                if not parsed_result:
                    logger.warning("  WARNING: Failed to parse job %s, skipping to next...", job_id)
                    skipped += 1
                    continue

                # Evaluate
                evaluation_results = orchestrator.evaluate_cve_analysis(parsed_result, stages=args.stages)
                results.append(evaluation_results)
                successful += 1
                logger.info("  Job completed successfully")

            logger.info("")
            logger.info("=" * 80)
            logger.info("Local mode processing complete:")
            logger.info("   Total jobs: %d", len(jobs_to_process))
            logger.info("   Successful: %d", successful)
            logger.info("   Skipped: %d", skipped)
            logger.info("=" * 80)

        # Save results to file
        logger.info("")
        logger.info("Saving results to %s...", args.output)
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # Convert to API format if requested
        if args.output_format == "api":
            logger.info("  Converting to API format (flat list of metrics)...")
            api_results = []
            for result in results:
                job_id = result.get("job_id", "")
                trace_id = result.get("trace_id", "unknown")
                execution_start_timestamp = result.get("execution_start_timestamp", "")
                cve_id = result.get("cve_id", "")
                component = result.get("component", "unknown")
                component_version = result.get("component_version", "unknown")

                for metric in result.get("metrics", []):
                    record = {
                        "job_id": job_id,
                        "trace_id": trace_id,
                        "execution_start_timestamp": execution_start_timestamp,
                        "cve": cve_id,
                        "component": component,
                        "component_version": component_version,
                        "llm_node": metric.get("llm_node"),
                        "metric_name": metric.get("metric_name"),
                        "metric_score": str(metric.get("metric_score", 0.0)),
                        "metric_reasoning": metric.get("metric_reasoning", "")
                    }

                    # Add model_input and model_output for AGENT_LOOP metrics
                    if metric.get("llm_node") == "AGENT_LOOP":
                        record["model_input"] = metric.get("model_input", "")
                        record["model_output"] = metric.get("model_output", "")

                    api_results.append(record)
            results_to_save = api_results
            logger.info("  Converted %d job results into %d API metrics", len(results), len(api_results))
        else:
            results_to_save = results
            logger.info("  Using local format (nested structure)")

        with open(output_path, 'w') as f:
            json.dump(results_to_save, f, indent=2)

        logger.info("")
        logger.info("=" * 80)
        logger.info("Pipeline complete!")
        logger.info("   Processed: %d jobs", len(results))
        logger.info("   Output format: %s", args.output_format)
        logger.info("   Results saved to: %s", args.output)
        if args.output_format == "api":
            logger.info(" Use 'jq .' to inspect the flat API format")
        logger.info("=" * 80)

        return 0

    except KeyboardInterrupt:
        logger.info("")
        logger.warning("WARNING: Interrupted by user")
        return 130
    except Exception as e:
        logger.error("")
        logger.error("ERROR: Pipeline failed: %s", e, exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))
