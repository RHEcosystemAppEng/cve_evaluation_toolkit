[project]
name = "cve-evaluation-toolkit"
version = "0.1.0"
description = "Automated evaluation toolkit for CVE analysis pipeline"
authors = [
    {name = "Your Team", email = "team@example.com"}
]
requires-python = ">=3.12"
readme = "README.md"
license = {text = "MIT"}

dependencies = [
    "httpx>=0.27.0",
    "pydantic>=2.0.0",
    "deepeval>=1.0.0",
    "openai>=1.0.0",
    "python-dotenv>=1.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.23.0",
    "pytest-cov>=4.0.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "mypy>=1.0.0",
]

[project.scripts]
eval = "scripts.run_evaluation:main"
parse-test = "scripts.test_data_parser:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["evaluation"]

[tool.black]
line-length = 120
target-version = ['py312']

[tool.ruff]
line-length = 120
target-version = "py312"
select = ["E", "F", "I", "N", "W"]
ignore = ["E501"]

[tool.mypy]
python_version = "3.12"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = false

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "-v --cov=evaluation --cov-report=term-missing"