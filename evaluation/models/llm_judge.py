# SPDX-FileCopyrightText: Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Base LLM Judge for evaluation metrics.

Supports Fireworks API, DeepEval, and various LLM providers.
"""

import os
from typing import Any
from typing import Dict
from typing import List
from typing import Optional

from deepeval.models.base_model import DeepEvalBaseLLM
from openai import OpenAI
from pydantic import BaseModel
from pydantic import Field

# Logger with compatibility for both old and new structure
try:
    from evaluation.utils.logger import get_logger
    logger = get_logger(__name__)
except ImportError:
    # Fallback to vuln_analysis logger
    from vuln_analysis.logging.loggers_factory import LoggingFactory
    logger = LoggingFactory.get_agent_logger(__name__)


class JudgeConfig(BaseModel):
    """
    Configuration for LLM Judge.

    Follows Pydantic patterns used throughout vuln_analysis.
    """
    model_name: str = Field(
        # default="nvidia/nemotron-3-nano",
        default="mistralai/mistral-small-3.1-24b-instruct-2503",
        description="Model name/path for the judge LLM")
    api_key: Optional[str] = Field(default=None, description="API key (defaults to NGC_API_KEY env var)")
    base_url: str = Field(default="https://integrate.api.nvidia.com/v1", description="API base URL (NVIDIA NIM)")
    temperature: float = Field(default=0.0, ge=0.0, le=2.0, description="Temperature for generation")
    max_tokens: int = Field(default=2048, gt=0, description="Maximum tokens for generation")

    class Config:
        """Pydantic configuration."""
        extra = "allow"


class FireworksJudge(DeepEvalBaseLLM):
    """
    LLM Judge using NVIDIA NIM API.

    Compatible with DeepEval's metric system.
    Note: Class name kept as FireworksJudge for backward compatibility.

    Example:
        >>> judge = FireworksJudge()
        >>> response = judge.generate("Evaluate this text...")
    """

    def __init__(self, config: Optional[JudgeConfig] = None):
        """
        Initialize the NVIDIA NIM judge.

        Args:
            config: Optional configuration (uses defaults if not provided)
        """
        self.config = config or JudgeConfig()
        api_key = self.config.api_key or os.environ.get("NGC_API_KEY") or os.environ.get("FIREWORKS_API_KEY")

        if not api_key:
            logger.warning("No API key provided - judge will fail on generate()")
            logger.warning("Please set NGC_API_KEY environment variable")

        self.client = OpenAI(api_key=api_key, base_url=self.config.base_url)
        logger.debug(f"Initialized Judge with model: {self.config.model_name} at {self.config.base_url}")

    def load_model(self):
        """Return the OpenAI client."""
        return self.client

    def generate(self, prompt: str) -> str:
        """
        Synchronous generation.

        Args:
            prompt: The prompt to send to the model

        Returns:
            Generated text response
        """
        logger.debug(f"Generating response (prompt length: {len(prompt)})")
        response = self.client.chat.completions.create(model=self.config.model_name,
                                                       messages=[{
                                                           "role": "user", "content": prompt
                                                       }],
                                                       temperature=self.config.temperature,
                                                       max_tokens=self.config.max_tokens)
        content = response.choices[0].message.content

        # Handle thinking tags (if model uses them)
        if content and "<think>" in content and "</think>" in content:
            content = content.split("</think>")[-1].strip()
            logger.debug("Stripped thinking tags from response")

        return content or ""

    async def a_generate(self, prompt: str) -> str:
        """
        Async generation (uses sync for simplicity).

        Args:
            prompt: The prompt to send to the model

        Returns:
            Generated text response
        """
        return self.generate(prompt)

    def get_model_name(self) -> str:
        """Return the model name."""
        return self.config.model_name


class DirectJudge:
    """
    Direct LLM Judge without DeepEval wrapper.

    For custom evaluations that don't fit DeepEval's framework.
    """

    def __init__(self, config: Optional[JudgeConfig] = None):
        """
        Initialize the direct judge.

        Args:
            config: Optional configuration (uses defaults if not provided)
        """
        self.config = config or JudgeConfig()
        api_key = self.config.api_key or os.environ.get("NGC_API_KEY") or os.environ.get("FIREWORKS_API_KEY")

        self.client = OpenAI(api_key=api_key, base_url=self.config.base_url)
        logger.debug(f"Initialized DirectJudge with model: {self.config.model_name}")

    def judge(self, prompt: str, system_prompt: Optional[str] = None) -> Dict[str, Any]:
        """
        Execute a judgment with structured output.

        Args:
            prompt: User prompt for the judgment
            system_prompt: Optional system prompt

        Returns:
            Dict with raw_response, thinking (if any), and usage stats
        """
        messages: List[Dict[str, str]] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        logger.debug(f"Executing judgment (prompt length: {len(prompt)})")
        response = self.client.chat.completions.create(model=self.config.model_name,
                                                       messages=messages,
                                                       temperature=self.config.temperature,
                                                       max_tokens=self.config.max_tokens)

        content = response.choices[0].message.content

        # Parse thinking tags if present
        thinking = ""
        if "<think>" in content and "</think>" in content:
            thinking = content.split("<think>")[1].split("</think>")[0]
            content = content.split("</think>")[-1].strip()

        return {
            "raw_response": content,
            "thinking": thinking,
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            }
        }


def get_judge(judge_type: str = "fireworks", **kwargs) -> DeepEvalBaseLLM:
    """
    Factory function to create LLM judges.

    Args:
        judge_type: Type of judge ("fireworks" or "direct")
        **kwargs: Additional configuration parameters

    Returns:
        Configured judge instance

    Raises:
        ValueError: If judge_type is unknown
    """
    logger.info(f"Creating judge of type: {judge_type}")

    if judge_type == "fireworks":
        return FireworksJudge(JudgeConfig(**kwargs))
    elif judge_type == "direct":
        return DirectJudge(JudgeConfig(**kwargs))
    else:
        raise ValueError(f"Unknown judge type: {judge_type}")
